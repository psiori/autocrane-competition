{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grapple model development\n",
    "example how to use dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from grapple_evaluation_helper import ValidationHelper\n",
    "from grapple_evaluation_helper import DataLoader as data_loader_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to splitted data\n",
    "path_to_train_images = '../data/Grapple/train/images'\n",
    "path_to_train_annotations = '../data/Grapple/train/annotations'\n",
    "\n",
    "path_to_test_images = '../data/Grapple/test/images'\n",
    "path_to_test_annotations = '../data/Grapple/test/annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrappleDataset(Dataset):\n",
    "    \"\"\"Grapple dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, annotations_dir, image_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotations_dir (string):directory with all the annotations.\n",
    "            image_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.imgs_names = sorted(os.listdir(image_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = imageio.imread(self.image_dir + \"/\" + self.imgs_names[idx])\n",
    "        coordinateBoundingbox = data_loader_helper().xml_to_coordinates_bounding_box( self.annotations_dir + \"/\" + self.imgs_names[idx].replace(\".png\", \".xml\"))\n",
    "        return np.array(img), np.array(coordinateBoundingbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataloaders\n",
    "dataset_train = GrappleDataset(path_to_train_annotations,path_to_test_images)\n",
    "loader_train = data.DataLoader(dataset_train, batch_size = 32, shuffle = True)\n",
    "\n",
    "dataset_test= GrappleDataset(path_to_test_annotations,path_to_test_images)\n",
    "loader_test = data.DataLoader(dataset_test, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
